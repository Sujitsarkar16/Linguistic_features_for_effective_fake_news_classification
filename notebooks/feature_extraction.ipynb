{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from readability import Readability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PREPROCESSED_DATA_FILE = \"cleaned_news.csv\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + PREPROCESSED_DATA_FILE, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Years...</td>\n",
       "      <td>['Donald', 'Trump', 'wish', 'Americans', 'Happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>['House', 'Intelligence', 'Committee', 'Chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>['Friday', 'revealed', 'former', 'Milwaukee', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamas Name ...</td>\n",
       "      <td>['Christmas', 'day', 'Donald', 'Trump', 'annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>['Pope', 'Francis', 'used', 'annual', 'Christm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>real</td>\n",
       "      <td>Fully committed NATO backs new US approach on ...</td>\n",
       "      <td>['BRUSSELS', 'Reuters', 'NATO', 'allies', 'Tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>real</td>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>['LONDON', 'Reuters', 'LexisNexis', 'provider'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>real</td>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>['MINSK', 'Reuters', 'shadow', 'disused', 'Sov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>real</td>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>['MOSCOW', 'Reuters', 'Vatican', 'Secretary', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>real</td>\n",
       "      <td>Indonesia to buy 114 billion worth of Russian ...</td>\n",
       "      <td>['JAKARTA', 'Reuters', 'Indonesia', 'buy', '11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44893  'Fully committed' NATO backs new U.S. approach...   \n",
       "44894  LexisNexis withdrew two products from Chinese ...   \n",
       "44895  Minsk cultural hub becomes haven from authorities   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44897  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "44894  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "44895  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "44896  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date label  \\\n",
       "0      December 31, 2017  fake   \n",
       "1      December 31, 2017  fake   \n",
       "2      December 30, 2017  fake   \n",
       "3      December 29, 2017  fake   \n",
       "4      December 25, 2017  fake   \n",
       "...                  ...   ...   \n",
       "44893   August 22, 2017   real   \n",
       "44894   August 22, 2017   real   \n",
       "44895   August 22, 2017   real   \n",
       "44896   August 22, 2017   real   \n",
       "44897   August 22, 2017   real   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Years...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obamas Name ...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44893  Fully committed NATO backs new US approach on ...   \n",
       "44894  LexisNexis withdrew two products from Chinese ...   \n",
       "44895  Minsk cultural hub becomes haven from authorities   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44897  Indonesia to buy 114 billion worth of Russian ...   \n",
       "\n",
       "                                     tokens_no_stopwords  \n",
       "0      ['Donald', 'Trump', 'wish', 'Americans', 'Happ...  \n",
       "1      ['House', 'Intelligence', 'Committee', 'Chairm...  \n",
       "2      ['Friday', 'revealed', 'former', 'Milwaukee', ...  \n",
       "3      ['Christmas', 'day', 'Donald', 'Trump', 'annou...  \n",
       "4      ['Pope', 'Francis', 'used', 'annual', 'Christm...  \n",
       "...                                                  ...  \n",
       "44893  ['BRUSSELS', 'Reuters', 'NATO', 'allies', 'Tue...  \n",
       "44894  ['LONDON', 'Reuters', 'LexisNexis', 'provider'...  \n",
       "44895  ['MINSK', 'Reuters', 'shadow', 'disused', 'Sov...  \n",
       "44896  ['MOSCOW', 'Reuters', 'Vatican', 'Secretary', ...  \n",
       "44897  ['JAKARTA', 'Reuters', 'Indonesia', 'buy', '11...  \n",
       "\n",
       "[44898 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['text','date','subject'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Years...</td>\n",
       "      <td>['Donald', 'Trump', 'wish', 'Americans', 'Happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>['House', 'Intelligence', 'Committee', 'Chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>['Friday', 'revealed', 'former', 'Milwaukee', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamas Name ...</td>\n",
       "      <td>['Christmas', 'day', 'Donald', 'Trump', 'annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>['Pope', 'Francis', 'used', 'annual', 'Christm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>real</td>\n",
       "      <td>Fully committed NATO backs new US approach on ...</td>\n",
       "      <td>['BRUSSELS', 'Reuters', 'NATO', 'allies', 'Tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>real</td>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>['LONDON', 'Reuters', 'LexisNexis', 'provider'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>real</td>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>['MINSK', 'Reuters', 'shadow', 'disused', 'Sov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>real</td>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>['MOSCOW', 'Reuters', 'Vatican', 'Secretary', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>real</td>\n",
       "      <td>Indonesia to buy 114 billion worth of Russian ...</td>\n",
       "      <td>['JAKARTA', 'Reuters', 'Indonesia', 'buy', '11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title label  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...  fake   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...  fake   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...  fake   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...  fake   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...  fake   \n",
       "...                                                  ...   ...   \n",
       "44893  'Fully committed' NATO backs new U.S. approach...  real   \n",
       "44894  LexisNexis withdrew two products from Chinese ...  real   \n",
       "44895  Minsk cultural hub becomes haven from authorities  real   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...  real   \n",
       "44897  Indonesia to buy $1.14 billion worth of Russia...  real   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Years...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obamas Name ...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44893  Fully committed NATO backs new US approach on ...   \n",
       "44894  LexisNexis withdrew two products from Chinese ...   \n",
       "44895  Minsk cultural hub becomes haven from authorities   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44897  Indonesia to buy 114 billion worth of Russian ...   \n",
       "\n",
       "                                     tokens_no_stopwords  \n",
       "0      ['Donald', 'Trump', 'wish', 'Americans', 'Happ...  \n",
       "1      ['House', 'Intelligence', 'Committee', 'Chairm...  \n",
       "2      ['Friday', 'revealed', 'former', 'Milwaukee', ...  \n",
       "3      ['Christmas', 'day', 'Donald', 'Trump', 'annou...  \n",
       "4      ['Pope', 'Francis', 'used', 'annual', 'Christm...  \n",
       "...                                                  ...  \n",
       "44893  ['BRUSSELS', 'Reuters', 'NATO', 'allies', 'Tue...  \n",
       "44894  ['LONDON', 'Reuters', 'LexisNexis', 'provider'...  \n",
       "44895  ['MINSK', 'Reuters', 'shadow', 'disused', 'Sov...  \n",
       "44896  ['MOSCOW', 'Reuters', 'Vatican', 'Secretary', ...  \n",
       "44897  ['JAKARTA', 'Reuters', 'Indonesia', 'buy', '11...  \n",
       "\n",
       "[44898 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_per_word(text):\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_paragraphs(text):\n",
    "    return text.count('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    return len(TextBlob(text).sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_types(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_per_sentence(text):\n",
    "    sentences = TextBlob(text).sentences\n",
    "    if sentences:\n",
    "        return sum(len(sentence.words) for sentence in sentences) / len(sentences)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(text):\n",
    "    words = text.split()\n",
    "    types = set(words)\n",
    "    if words:\n",
    "        return len(types) / len(words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Letters per Word'] = df['cleaned_text'].apply(letters_per_word)\n",
    "df['Number of Paragraphs'] = df['cleaned_text'].apply(count_paragraphs)\n",
    "df['Number of Sentences'] = df['cleaned_text'].apply(count_sentences)\n",
    "df['Number of Types'] = df['cleaned_text'].apply(count_types)\n",
    "df['Number of Words'] = df['cleaned_text'].apply(count_words)\n",
    "df['Number of Words per Sentence'] = df['cleaned_text'].apply(words_per_sentence)\n",
    "df['TTR'] = df['cleaned_text'].apply(type_token_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counts(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    pos_counts = pd.Series([tag for word, tag in pos_tags]).value_counts()\n",
    "    return pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = df['cleaned_text'].apply(pos_counts).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NNP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>RP</th>\n",
       "      <th>NN</th>\n",
       "      <th>DT</th>\n",
       "      <th>VBG</th>\n",
       "      <th>IN</th>\n",
       "      <th>TO</th>\n",
       "      <th>RB</th>\n",
       "      <th>JJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RBR</th>\n",
       "      <th>WP</th>\n",
       "      <th>WDT</th>\n",
       "      <th>UH</th>\n",
       "      <th>PDT</th>\n",
       "      <th>$</th>\n",
       "      <th>FW</th>\n",
       "      <th>''</th>\n",
       "      <th>POS</th>\n",
       "      <th>WP$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NNP  VBZ  RP  NN  DT  VBG  IN  TO  RB  JJ  ...  RBR  WP  WDT  UH  PDT  \\\n",
       "0        6    2   1   1   1    1   0   0   0   0  ...    0   0    0   0    0   \n",
       "1        8    0   0   0   0    0   0   0   0   0  ...    0   0    0   0    0   \n",
       "2        8    0   0   1   2    1   2   1   0   0  ...    0   0    0   0    0   \n",
       "3        6    2   0   0   0    0   0   0   2   1  ...    0   0    0   0    0   \n",
       "4        7    0   1   0   0    0   1   0   0   0  ...    0   0    0   0    0   \n",
       "...    ...  ...  ..  ..  ..  ...  ..  ..  ..  ..  ...  ...  ..  ...  ..  ...   \n",
       "44893    3    1   0   1   0    0   1   0   1   1  ...    0   0    0   0    0   \n",
       "44894    1    0   0   1   0    0   1   0   0   1  ...    0   0    0   0    0   \n",
       "44895    1    0   0   1   0    0   1   0   1   1  ...    0   0    0   0    0   \n",
       "44896    3    0   0   2   0    1   2   0   0   1  ...    0   0    0   0    0   \n",
       "44897    0    0   0   2   0    0   1   1   0   1  ...    0   0    0   0    0   \n",
       "\n",
       "       $  FW  ''  POS  WP$  \n",
       "0      0   0   0    0    0  \n",
       "1      0   0   0    0    0  \n",
       "2      0   0   0    0    0  \n",
       "3      0   0   0    0    0  \n",
       "4      0   0   0    0    0  \n",
       "...   ..  ..  ..  ...  ...  \n",
       "44893  0   0   0    0    0  \n",
       "44894  0   0   0    0    0  \n",
       "44895  0   0   0    0    0  \n",
       "44896  0   0   0    0    0  \n",
       "44897  0   0   0    0    0  \n",
       "\n",
       "[44898 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectors_add_info = ['and', 'also', 'besides', 'furthermore', 'in addition', 'moreover', 'as well as']\n",
    "connectors_comparison = ['similarly', 'likewise', 'equally']\n",
    "connectors_contrast = ['but', 'however', 'on the other hand', 'nevertheless', 'nonetheless', 'whereas', 'while', 'conversely']\n",
    "connectors_emphasis = ['especially', 'particularly', 'in particular', 'notably', 'above all']\n",
    "connectors_explain = ['for example', 'for instance', 'such as', 'including', 'namely']\n",
    "connectors_expressing_facts = ['actually', 'in fact', 'indeed']\n",
    "connectors_expressing_opinion = ['in my opinion', 'I believe', 'I think', 'personally']\n",
    "connectors_reason_cause = ['because', 'since', 'as', 'due to', 'owing to', 'the reason why']\n",
    "connectors_time_sequence = ['first', 'second', 'then', 'next', 'finally', 'after', 'afterward', 'before', 'previously', 'subsequently']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_connectors(text):\n",
    "    counts = {\n",
    "        'add_info':0,\n",
    "        'comparison': 0,\n",
    "        'contrast': 0,\n",
    "        'emphasis': 0,\n",
    "        'explain': 0,\n",
    "        'expressing_facts': 0,\n",
    "        'expressing_opinion': 0,\n",
    "        'reason_cause': 0,\n",
    "        'time_sequence': 0\n",
    "    }\n",
    "\n",
    "    for word in re.findall(r\"\\b\\w+\\b\", text.lower()):\n",
    "        if word in connectors_add_info:\n",
    "            counts['add_info'] += 1\n",
    "        if word in connectors_comparison:\n",
    "            counts['comparison'] += 1\n",
    "        if word in connectors_contrast:\n",
    "            counts['contrast'] += 1\n",
    "        if word in connectors_emphasis:\n",
    "            counts['emphasis'] += 1\n",
    "        if word in connectors_explain:\n",
    "            counts['explain'] += 1\n",
    "        if word in connectors_expressing_facts:\n",
    "            counts['expressing_facts'] += 1\n",
    "        if word in connectors_expressing_opinion:\n",
    "            counts['expressing_opinion'] += 1\n",
    "        if word in connectors_reason_cause:\n",
    "            counts['reason_cause'] += 1\n",
    "        if word in connectors_time_sequence:\n",
    "            counts['time_sequence'] += 1\n",
    "    return pd.Series(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_features = df['cleaned_text'].apply(count_connectors)\n",
    "df = pd.concat([df, connector_features], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discursive characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discursive_char(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    determiners = sum(1 for word, tag in tagged_tokens if tag == 'DT')\n",
    "    pronouns = sum(1 for word, tag in tagged_tokens if 'PRP' in tag)\n",
    "    first_person_pronouns = sum(1 for word in tokens if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'])\n",
    "    positive_words = sum(1 for word in tokens if word.lower() in ['not', 'no', 'never', 'none', \"isn't\", \"wasn't\", \"aren't\", \"won't\", \"didn't\", \"don't\", \"doesn't\", \"haven't\", \"hasn't\"] )\n",
    "    negative_words = sum(1 for word in tokens if word.lower() in ['bad', 'worse', 'negative', 'sad'])\n",
    "    negations = sum(1 for word in tokens if word.lower() in ['not', 'no', 'never', 'none', \"isn't\", \"wasn't\", \"aren't\", \"won't\", \"didn't\", \"don't\", \"doesn't\", \"haven't\", \"hasn't\"])\n",
    "    second_person_pronouns = sum(1 for word in tokens if word.lower() in ['you', 'your', 'yours', 'yourself', 'yourselves'])\n",
    "    third_person_pronouns = sum(1 for word in tokens if word.lower() in ['he', 'she', 'it', 'him', 'her', 'his', 'hers', 'its', 'himself', 'herself', 'itself', 'they', 'them', 'their', 'theirs', 'themselves'])\n",
    "    swear_words = sum(1 for word in tokens if word.lower() in ['shit', \"fck\", \"fc**k\", \"bitch\", \"st\" ])\n",
    "    discourse_markers = sum(1 for word in tokens if word.lower() in ['however', 'furthermore', 'moreover', 'therefore', 'thus', 'consequently', 'hence'])\n",
    "    key_connectors = sum(1 for word in tokens if word.lower() in ['and', 'but', 'or', 'so', 'yet', 'for', 'nor'])\n",
    "    future_regex = re.compile(r'\\bwill\\b|\\bshall\\b', re.IGNORECASE)\n",
    "    mechanism_words = sum(1 for word in tokens if word.lower() in ['process', 'method', 'system', 'approach', 'technique'])\n",
    "    LIWC_pronouns = sum(1 for word in tokens if word.lower() in ['I', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'] )\n",
    "    LIWC_psychological = sum(1 for word in tokens if word.lower() in ['think', 'believe', 'understand', 'know', 'consider', 'remember', 'imagine'])\n",
    "\n",
    "\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "    cli = textstat.coleman_liau_index(text)\n",
    "\n",
    "    features = {\n",
    "    'Determiners': determiners,\n",
    "        'Pronouns': pronouns,\n",
    "        'First Person Pronouns': first_person_pronouns,\n",
    "        'Negations': negations,\n",
    "        'Positive Words': positive_words,\n",
    "        'Negative Words': negative_words,\n",
    "        'ARI': ari,\n",
    "        'CLI': cli,\n",
    "        'SPP' : second_person_pronouns,\n",
    "        'TPP' : third_person_pronouns,\n",
    "        'Swear' : swear_words,\n",
    "        'Discourse_markers': discourse_markers,\n",
    "        'Key_conectors': key_connectors,\n",
    "        'Future_regex': future_regex,\n",
    "        'Mechanism': mechanism_words,\n",
    "        'LIWC_pronouns': LIWC_pronouns,\n",
    "        'LIWC_psychological': LIWC_psychological\n",
    "\n",
    "\n",
    "}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_of_art(text):\n",
    "    state_of_art = ['state of art', 'cutting-edge', 'latest', 'innovative', 'advanced']\n",
    "    pattern = '|'.join(state_of_art)\n",
    "    return len(re.findall(pattern, text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_summary_phrases(text):\n",
    "    summary_phrases = ['in summary', 'to summarize', 'overall', 'in conclusion']\n",
    "    pattern = '|'.join(summary_phrases)\n",
    "    return len(re.findall(pattern, text.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State of the Art Phrases'] = df['cleaned_text'].apply(state_of_art)\n",
    "df['Summary Phrases'] = df['cleaned_text'].apply(count_summary_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_and_summary = ['State of the Art Phrases', 'Summary Phrases']\n",
    "\n",
    "state_and_summary_df = df[state_and_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = df['cleaned_text'].apply(lambda text: pd.Series(Discursive_char(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Determiners</th>\n",
       "      <th>Pronouns</th>\n",
       "      <th>First Person Pronouns</th>\n",
       "      <th>Negations</th>\n",
       "      <th>Positive Words</th>\n",
       "      <th>Negative Words</th>\n",
       "      <th>ARI</th>\n",
       "      <th>CLI</th>\n",
       "      <th>SPP</th>\n",
       "      <th>TPP</th>\n",
       "      <th>Swear</th>\n",
       "      <th>Discourse_markers</th>\n",
       "      <th>Key_conectors</th>\n",
       "      <th>Future_regex</th>\n",
       "      <th>Mechanism</th>\n",
       "      <th>LIWC_pronouns</th>\n",
       "      <th>LIWC_psychological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>24.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>12.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>15.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Determiners  Pronouns  First Person Pronouns  Negations  \\\n",
       "0                1         0                      0          0   \n",
       "1                0         0                      0          0   \n",
       "2                2         0                      0          0   \n",
       "3                0         2                      0          0   \n",
       "4                0         1                      0          0   \n",
       "...            ...       ...                    ...        ...   \n",
       "44893            0         0                      1          0   \n",
       "44894            0         0                      0          0   \n",
       "44895            0         0                      0          0   \n",
       "44896            0         0                      0          0   \n",
       "44897            0         0                      0          0   \n",
       "\n",
       "       Positive Words  Negative Words   ARI    CLI  SPP  TPP  Swear  \\\n",
       "0                   0               0  10.1  13.27    0    0      0   \n",
       "1                   0               0  18.5  24.61    0    0      0   \n",
       "2                   0               0   9.0  10.37    0    0      0   \n",
       "3                   0               0   6.1   7.42    0    2      0   \n",
       "4                   0               0   9.3  12.62    0    1      0   \n",
       "...               ...             ...   ...    ...  ...  ...    ...   \n",
       "44893               0               0   8.7  12.50    0    0      0   \n",
       "44894               0               0  13.0  18.16    0    0      0   \n",
       "44895               0               0  11.0  15.67    0    0      0   \n",
       "44896               0               0  10.8  15.11    0    0      0   \n",
       "44897               0               0   5.1   8.03    0    0      0   \n",
       "\n",
       "       Discourse_markers  Key_conectors  \\\n",
       "0                      0              0   \n",
       "1                      0              0   \n",
       "2                      0              1   \n",
       "3                      0              1   \n",
       "4                      0              0   \n",
       "...                  ...            ...   \n",
       "44893                  0              0   \n",
       "44894                  0              0   \n",
       "44895                  0              0   \n",
       "44896                  0              0   \n",
       "44897                  0              0   \n",
       "\n",
       "                                            Future_regex  Mechanism  \\\n",
       "0      re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "1      re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "2      re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "3      re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "4      re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "...                                                  ...        ...   \n",
       "44893  re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          1   \n",
       "44894  re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "44895  re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "44896  re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "44897  re.compile('\\\\bwill\\\\b|\\\\bshall\\\\b', re.IGNORE...          0   \n",
       "\n",
       "       LIWC_pronouns  LIWC_psychological  \n",
       "0                  0                   0  \n",
       "1                  0                   0  \n",
       "2                  0                   0  \n",
       "3                  2                   0  \n",
       "4                  1                   0  \n",
       "...              ...                 ...  \n",
       "44893              1                   0  \n",
       "44894              0                   0  \n",
       "44895              0                   0  \n",
       "44896              0                   0  \n",
       "44897              0                   0  \n",
       "\n",
       "[44898 rows x 17 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_reading_ease, flesch_kincaid_grade, gunning_fog, smog_index, coleman_liau_index, dale_chall_readability_score, difficult_words, linsear_write_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_scores(text):\n",
    "    scores = {\n",
    "        'Flesch Reading Ease': flesch_reading_ease(text),\n",
    "        'Flesch-Kincaid Grade Level': flesch_kincaid_grade(text),\n",
    "        'Gunning Fog Index': gunning_fog(text),\n",
    "        'SMOG Index': smog_index(text),\n",
    "        'Coleman-Liau Index': coleman_liau_index(text),\n",
    "        'Dale-chall-readability': dale_chall_readability_score(text),\n",
    "        'difficult_words': difficult_words(text),\n",
    "        'Linsear_write_formula': linsear_write_formula(text)\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores = df['cleaned_text'].apply(calculate_readability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_scores_df = pd.json_normalize(readability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([df, readability_scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>Letters per Word</th>\n",
       "      <th>Number of Paragraphs</th>\n",
       "      <th>Number of Sentences</th>\n",
       "      <th>Number of Types</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Number of Words per Sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>State of the Art Phrases</th>\n",
       "      <th>Summary Phrases</th>\n",
       "      <th>Flesch Reading Ease</th>\n",
       "      <th>Flesch-Kincaid Grade Level</th>\n",
       "      <th>Gunning Fog Index</th>\n",
       "      <th>SMOG Index</th>\n",
       "      <th>Coleman-Liau Index</th>\n",
       "      <th>Dale-chall-readability</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>Linsear_write_formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Years...</td>\n",
       "      <td>['Donald', 'Trump', 'wish', 'Americans', 'Happ...</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.27</td>\n",
       "      <td>12.13</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>['House', 'Intelligence', 'Committee', 'Chairm...</td>\n",
       "      <td>7.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.06</td>\n",
       "      <td>12.3</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.61</td>\n",
       "      <td>17.85</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>['Friday', 'revealed', 'former', 'Milwaukee', ...</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.71</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.37</td>\n",
       "      <td>10.70</td>\n",
       "      <td>4</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamas Name ...</td>\n",
       "      <td>['Christmas', 'day', 'Donald', 'Trump', 'annou...</td>\n",
       "      <td>4.357143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74.19</td>\n",
       "      <td>6.4</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.42</td>\n",
       "      <td>11.10</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>['Pope', 'Francis', 'used', 'annual', 'Christm...</td>\n",
       "      <td>5.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.23</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.62</td>\n",
       "      <td>11.36</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...  fake   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...  fake   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...  fake   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...  fake   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...  fake   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Years...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamas Name ...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                 tokens_no_stopwords  Letters per Word  \\\n",
       "0  ['Donald', 'Trump', 'wish', 'Americans', 'Happ...          5.416667   \n",
       "1  ['House', 'Intelligence', 'Committee', 'Chairm...          7.625000   \n",
       "2  ['Friday', 'revealed', 'former', 'Milwaukee', ...          4.866667   \n",
       "3  ['Christmas', 'day', 'Donald', 'Trump', 'annou...          4.357143   \n",
       "4  ['Pope', 'Francis', 'used', 'annual', 'Christm...          5.363636   \n",
       "\n",
       "   Number of Paragraphs  Number of Sentences  Number of Types  \\\n",
       "0                     0                    1               12   \n",
       "1                     0                    1                8   \n",
       "2                     0                    1               15   \n",
       "3                     0                    1               14   \n",
       "4                     0                    1               11   \n",
       "\n",
       "   Number of Words  Number of Words per Sentence  ...  \\\n",
       "0               12                          12.0  ...   \n",
       "1                8                           8.0  ...   \n",
       "2               15                          15.0  ...   \n",
       "3               14                          14.0  ...   \n",
       "4               11                          11.0  ...   \n",
       "\n",
       "   State of the Art Phrases  Summary Phrases  Flesch Reading Ease  \\\n",
       "0                         0                0                59.30   \n",
       "1                         0                0                21.06   \n",
       "2                         0                0                64.71   \n",
       "3                         0                0                74.19   \n",
       "4                         0                0                77.23   \n",
       "\n",
       "   Flesch-Kincaid Grade Level  Gunning Fog Index  SMOG Index  \\\n",
       "0                         8.0              11.47         0.0   \n",
       "1                        12.3              13.20         0.0   \n",
       "2                         8.0              11.33         0.0   \n",
       "3                         6.4               5.60         0.0   \n",
       "4                         5.2               4.40         0.0   \n",
       "\n",
       "   Coleman-Liau Index  Dale-chall-readability  difficult_words  \\\n",
       "0               13.27                   12.13                3   \n",
       "1               24.61                   17.85                5   \n",
       "2               10.37                   10.70                4   \n",
       "3                7.42                   11.10                5   \n",
       "4               12.62                   11.36                2   \n",
       "\n",
       "   Linsear_write_formula  \n",
       "0                    7.0  \n",
       "1                    5.0  \n",
       "2                    8.5  \n",
       "3                    6.0  \n",
       "4                    4.5  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Letters per Word</th>\n",
       "      <th>Number of Paragraphs</th>\n",
       "      <th>Number of Sentences</th>\n",
       "      <th>Number of Types</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Number of Words per Sentence</th>\n",
       "      <th>TTR</th>\n",
       "      <th>Determiners</th>\n",
       "      <th>Pronouns</th>\n",
       "      <th>First Person Pronouns</th>\n",
       "      <th>...</th>\n",
       "      <th>State of the Art Phrases</th>\n",
       "      <th>Summary Phrases</th>\n",
       "      <th>Flesch Reading Ease</th>\n",
       "      <th>Flesch-Kincaid Grade Level</th>\n",
       "      <th>Gunning Fog Index</th>\n",
       "      <th>SMOG Index</th>\n",
       "      <th>Coleman-Liau Index</th>\n",
       "      <th>Dale-chall-readability</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>Linsear_write_formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.27</td>\n",
       "      <td>12.13</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.06</td>\n",
       "      <td>12.3</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.61</td>\n",
       "      <td>17.85</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.866667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.71</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.37</td>\n",
       "      <td>10.70</td>\n",
       "      <td>4</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.357143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74.19</td>\n",
       "      <td>6.4</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.42</td>\n",
       "      <td>11.10</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.23</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.62</td>\n",
       "      <td>11.36</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>5.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.34</td>\n",
       "      <td>6.8</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>12.86</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>6.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.53</td>\n",
       "      <td>10.7</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.16</td>\n",
       "      <td>13.01</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>6.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.53</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.67</td>\n",
       "      <td>17.52</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>5.888889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.58</td>\n",
       "      <td>13.9</td>\n",
       "      <td>16.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.11</td>\n",
       "      <td>16.36</td>\n",
       "      <td>6</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.26</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.03</td>\n",
       "      <td>12.86</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Letters per Word  Number of Paragraphs  Number of Sentences  \\\n",
       "0              5.416667                     0                    1   \n",
       "1              7.625000                     0                    1   \n",
       "2              4.866667                     0                    1   \n",
       "3              4.357143                     0                    1   \n",
       "4              5.363636                     0                    1   \n",
       "...                 ...                   ...                  ...   \n",
       "44893          5.444444                     0                    1   \n",
       "44894          6.571429                     0                    1   \n",
       "44895          6.142857                     0                    1   \n",
       "44896          5.888889                     0                    1   \n",
       "44897          4.666667                     0                    1   \n",
       "\n",
       "       Number of Types  Number of Words  Number of Words per Sentence  TTR  \\\n",
       "0                   12               12                          12.0  1.0   \n",
       "1                    8                8                           8.0  1.0   \n",
       "2                   15               15                          15.0  1.0   \n",
       "3                   14               14                          14.0  1.0   \n",
       "4                   11               11                          11.0  1.0   \n",
       "...                ...              ...                           ...  ...   \n",
       "44893                9                9                           9.0  1.0   \n",
       "44894                7                7                           7.0  1.0   \n",
       "44895                7                7                           7.0  1.0   \n",
       "44896                9                9                           9.0  1.0   \n",
       "44897                9                9                           9.0  1.0   \n",
       "\n",
       "       Determiners  Pronouns  First Person Pronouns  ...  \\\n",
       "0                1         0                      0  ...   \n",
       "1                0         0                      0  ...   \n",
       "2                2         0                      0  ...   \n",
       "3                0         2                      0  ...   \n",
       "4                0         1                      0  ...   \n",
       "...            ...       ...                    ...  ...   \n",
       "44893            0         0                      1  ...   \n",
       "44894            0         0                      0  ...   \n",
       "44895            0         0                      0  ...   \n",
       "44896            0         0                      0  ...   \n",
       "44897            0         0                      0  ...   \n",
       "\n",
       "       State of the Art Phrases  Summary Phrases  Flesch Reading Ease  \\\n",
       "0                             0                0                59.30   \n",
       "1                             0                0                21.06   \n",
       "2                             0                0                64.71   \n",
       "3                             0                0                74.19   \n",
       "4                             0                0                77.23   \n",
       "...                         ...              ...                  ...   \n",
       "44893                         0                0                62.34   \n",
       "44894                         0                0                30.53   \n",
       "44895                         0                0                30.53   \n",
       "44896                         0                0                11.58   \n",
       "44897                         0                0                79.26   \n",
       "\n",
       "       Flesch-Kincaid Grade Level  Gunning Fog Index  SMOG Index  \\\n",
       "0                             8.0              11.47         0.0   \n",
       "1                            12.3              13.20         0.0   \n",
       "2                             8.0              11.33         0.0   \n",
       "3                             6.4               5.60         0.0   \n",
       "4                             5.2               4.40         0.0   \n",
       "...                           ...                ...         ...   \n",
       "44893                         6.8               8.04         0.0   \n",
       "44894                        10.7               8.51         0.0   \n",
       "44895                        10.7              14.23         0.0   \n",
       "44896                        13.9              16.93         0.0   \n",
       "44897                         4.4               8.04         0.0   \n",
       "\n",
       "       Coleman-Liau Index  Dale-chall-readability  difficult_words  \\\n",
       "0                   13.27                   12.13                3   \n",
       "1                   24.61                   17.85                5   \n",
       "2                   10.37                   10.70                4   \n",
       "3                    7.42                   11.10                5   \n",
       "4                   12.62                   11.36                2   \n",
       "...                   ...                     ...              ...   \n",
       "44893               12.50                   12.86                3   \n",
       "44894               18.16                   13.01                4   \n",
       "44895               15.67                   17.52                4   \n",
       "44896               15.11                   16.36                6   \n",
       "44897                8.03                   12.86                3   \n",
       "\n",
       "       Linsear_write_formula  \n",
       "0                        7.0  \n",
       "1                        5.0  \n",
       "2                        8.5  \n",
       "3                        6.0  \n",
       "4                        4.5  \n",
       "...                      ...  \n",
       "44893                    4.5  \n",
       "44894                    3.5  \n",
       "44895                    4.5  \n",
       "44896                    6.5  \n",
       "44897                    4.5  \n",
       "\n",
       "[44898 rows x 92 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "surface_info_columns = ['Letters per Word', 'Number of Paragraphs', 'Number of Sentences',\n",
    "                        'Number of Types', 'Number of Words', 'Number of Words per Sentence', 'TTR']\n",
    "\n",
    "surface_info_df = df[surface_info_columns]\n",
    "\n",
    "final_features_df = pd.concat([surface_info_df, features_df, pos_features, state_and_summary_df, combined_data], axis=1)\n",
    "\n",
    "final_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = ['cleaned_text', 'title', 'tokens_no_stopwords', 'Future_regex']\n",
    "df_1 = final_features_df.drop(columns=[col for col in Features if col in final_features_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Features to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'  \n",
    "FILENAME = 'Linguistic_features.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = DATA_PATH + FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
