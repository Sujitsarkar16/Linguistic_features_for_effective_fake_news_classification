{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from textstat.textstat import textstatistics\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    pos_counts = nltk.FreqDist(tag for _, tag in tagged)\n",
    "    readability = textstatistics()\n",
    "    features = {\n",
    "        'Pronouns': sum(1 for _, tag in tagged if tag in ['PRP', 'PRP$']),\n",
    "        'TO': pos_counts.get('TO', 0),\n",
    "        'Key_conectors': sum(1 for word in tokens if word.lower() in ['and', 'but', 'or', 'so']),\n",
    "        'Flesch_Kincaid_Grade_Level': readability.flesch_kincaid_grade(text),\n",
    "        'Flesch_Reading_Ease': readability.flesch_reading_ease(text),\n",
    "        'CLI': readability.coleman_liau_index(text),\n",
    "        'add_info': 0,\n",
    "        'Linsear_write_formula': readability.linsear_write_formula(text),\n",
    "        'Determiners': pos_counts.get('DT', 0),\n",
    "        'ARI': readability.automated_readability_index(text),\n",
    "        'Number_of_Words': len(tokens),\n",
    "        'LIWC_pronouns': 0,\n",
    "        'Negations': sum(1 for word in tokens if word.lower() in ['not', 'no', 'never', 'none']),\n",
    "        'NNP': pos_counts.get('NNP', 0),\n",
    "        'TPP': 0,\n",
    "        'PRP': pos_counts.get('PRP', 0),\n",
    "        'Positive_Words': 0,\n",
    "        'Coleman_Liau_Index': readability.coleman_liau_index(text),\n",
    "        'DT': pos_counts.get('DT', 0),\n",
    "        'RB': pos_counts.get('RB', 0),\n",
    "        'Number_of_Words_per_Sentence': np.mean([len(word_tokenize(sentence)) for sentence in sentences]),\n",
    "        'CC': pos_counts.get('CC', 0),\n",
    "        'Number_of_Types': len(set(tokens))\n",
    "    }\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = CountVectorizer(max_features=1000, ngram_range=(1,1), stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_transformer = FunctionTransformer(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('text', text_transformer, 'cleaned_text'),\n",
    "    ('other', other_transformer, None)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = df['cleaned_text'].apply(extract_features).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count = df['cleaned_text'].values\n",
    "X_custom_features = features_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X_count_vectorized = count_vectorizer.fit_transform(X_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.hstack((X_count_vectorized.toarray(), X_custom_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 1023)\n"
     ]
    }
   ],
   "source": [
    "print(X_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVEC model saved to: ../src/fnClassification/Models/cvec_1.pkl\n"
     ]
    }
   ],
   "source": [
    "classifier_location = \"../src/fnClassification/Models/\"\n",
    "os.makedirs(classifier_location, exist_ok=True)  \n",
    "model_filename = os.path.join(classifier_location, 'cvec_1.pkl')\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(count_vectorizer, file)\n",
    "\n",
    "print(f\"CVEC model saved to: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
